{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a754b9fa",
   "metadata": {},
   "source": [
    "# Feature Engineering for Fraud Detection\n",
    "\n",
    "## Objective\n",
    "Create meaningful features from cleaned datasets to improve fraud detection model performance for both e-commerce and credit card transactions.\n",
    "\n",
    "## Tasks\n",
    "1. Load cleaned datasets from EDA phase\n",
    "2. Create advanced time-based features\n",
    "3. Calculate transaction frequency and velocity features\n",
    "4. Create interaction and derived features\n",
    "5. Perform data transformation (scaling, encoding)\n",
    "6. Handle class imbalance using SMOTE\n",
    "7. Prepare final datasets for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c1dd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067808c",
   "metadata": {},
   "source": [
    "## Load Cleaned Datasets\n",
    "\n",
    "Load both cleaned datasets from the EDA phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53429b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LOADING CLEANED DATASETS\n",
      "==================================================\n",
      "üìä E-commerce data shape: (129146, 18)\n",
      "   Columns: 18\n",
      "   Memory: 17.74 MB\n",
      "üí≥ Credit card data shape: (283726, 31)\n",
      "   Columns: 31\n",
      "   Memory: 67.10 MB\n",
      "\n",
      "üîç E-COMMERCE DATA COLUMNS:\n",
      "['user_id', 'signup_time', 'purchase_time', 'purchase_value', 'device_id', 'source', 'browser', 'sex', 'age', 'ip_address', 'class', 'ip_address_int', 'lower_bound_ip_address', 'upper_bound_ip_address', 'country', 'purchase_hour', 'purchase_day', 'time_since_signup_hours']\n",
      "\n",
      "üîç CREDIT CARD DATA COLUMNS:\n",
      "['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']\n",
      "\n",
      "‚úÖ Both datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"LOADING CLEANED DATASETS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load cleaned e-commerce data\n",
    "df_ecom = pd.read_csv('../data/processed/fraud_data_cleaned.csv')\n",
    "print(f\"üìä E-commerce data shape: {df_ecom.shape}\")\n",
    "print(f\"   Columns: {len(df_ecom.columns)}\")\n",
    "print(f\"   Memory: {df_ecom.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Load cleaned credit card data\n",
    "df_cc = pd.read_csv('../data/processed/creditcard_cleaned.csv')\n",
    "print(f\"üí≥ Credit card data shape: {df_cc.shape}\")\n",
    "print(f\"   Columns: {len(df_cc.columns)}\")\n",
    "print(f\"   Memory: {df_cc.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nüîç E-COMMERCE DATA COLUMNS:\")\n",
    "print(df_ecom.columns.tolist())\n",
    "\n",
    "print(\"\\nüîç CREDIT CARD DATA COLUMNS:\")\n",
    "print(df_cc.columns.tolist())\n",
    "\n",
    "print(\"\\n‚úÖ Both datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a274811f",
   "metadata": {},
   "source": [
    "## Feature Engineering - E-commerce Data\n",
    "\n",
    "Based on the Task 1 requirements, we need to create:\n",
    "1. Transaction frequency and velocity features\n",
    "2. Additional time-based features\n",
    "3. Country risk encoding\n",
    "4. Device and user behavior patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d1a12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "FEATURE ENGINEERING - E-COMMERCE DATA\n",
      "==================================================\n",
      "Original shape: (129146, 18)\n",
      "\n",
      "üìÖ TIME-BASED FEATURES:\n",
      "------------------------------\n",
      "‚úÖ Created basic time features: purchase_month, purchase_day_of_month, purchase_minute, signup_hour, signup_day\n",
      "‚úÖ Created time_since_signup in days and weeks\n",
      "‚úÖ Created time_of_day categories and is_weekend flag\n",
      "\n",
      "üë§ USER BEHAVIOR FEATURES:\n",
      "------------------------------\n",
      "‚úÖ Created user transaction features: user_total_transactions, time_since_last_txn_hours\n",
      "‚úÖ Created device usage feature: device_usage_count\n",
      "\n",
      "üåç GEOGRAPHICAL & RISK FEATURES:\n",
      "------------------------------\n",
      "‚úÖ Created country risk features: country_fraud_rate, country_risk_category\n",
      "\n",
      "üí∞ PURCHASE BEHAVIOR FEATURES:\n",
      "------------------------------\n",
      "‚úÖ Created purchase value features: log transform, sqrt transform, categories\n",
      "‚úÖ Created age_group feature\n",
      "\n",
      "üîó INTERACTION FEATURES:\n",
      "------------------------------\n",
      "‚úÖ Created interaction features: is_new_user, high_value_new_user, unusual_hour_purchase\n",
      "\n",
      "üéØ FEATURE ENGINEERING SUMMARY:\n",
      "----------------------------------------\n",
      "Original features: 18\n",
      "After feature engineering: 39\n",
      "New features added: 21\n",
      "\n",
      "üìä New columns created:\n",
      "   1. age_group\n",
      "   2. country_fraud_rate\n",
      "   3. country_risk_category\n",
      "   4. device_usage_count\n",
      "   5. high_value_new_user\n",
      "   6. is_new_user\n",
      "   7. is_weekend\n",
      "   8. purchase_day_of_month\n",
      "   9. purchase_minute\n",
      "  10. purchase_month\n",
      "  11. purchase_value_category\n",
      "  12. purchase_value_log\n",
      "  13. purchase_value_sqrt\n",
      "  14. signup_day\n",
      "  15. signup_hour\n",
      "  16. time_of_day\n",
      "  17. time_since_last_txn_hours\n",
      "  18. time_since_signup_days\n",
      "  19. time_since_signup_weeks\n",
      "  20. unusual_hour_purchase\n",
      "  21. user_total_transactions\n",
      "\n",
      "‚úÖ E-commerce feature engineering completed!\n",
      "   Final shape: (129146, 39)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"FEATURE ENGINEERING - E-COMMERCE DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Make a copy for feature engineering\n",
    "df_ecom_fe = df_ecom.copy()\n",
    "\n",
    "print(f\"Original shape: {df_ecom_fe.shape}\")\n",
    "\n",
    "# Convert timestamp columns to datetime (if not already)\n",
    "df_ecom_fe['signup_time'] = pd.to_datetime(df_ecom_fe['signup_time'])\n",
    "df_ecom_fe['purchase_time'] = pd.to_datetime(df_ecom_fe['purchase_time'])\n",
    "\n",
    "print(\"\\nüìÖ TIME-BASED FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 1. Extract more detailed time features\n",
    "df_ecom_fe['purchase_month'] = df_ecom_fe['purchase_time'].dt.month\n",
    "df_ecom_fe['purchase_day_of_month'] = df_ecom_fe['purchase_time'].dt.day\n",
    "df_ecom_fe['purchase_minute'] = df_ecom_fe['purchase_time'].dt.minute\n",
    "df_ecom_fe['signup_hour'] = df_ecom_fe['signup_time'].dt.hour\n",
    "df_ecom_fe['signup_day'] = df_ecom_fe['signup_time'].dt.dayofweek\n",
    "\n",
    "print(\"‚úÖ Created basic time features: purchase_month, purchase_day_of_month, purchase_minute, signup_hour, signup_day\")\n",
    "\n",
    "# 2. Time since signup in different units\n",
    "df_ecom_fe['time_since_signup_days'] = df_ecom_fe['time_since_signup_hours'] / 24\n",
    "df_ecom_fe['time_since_signup_weeks'] = df_ecom_fe['time_since_signup_days'] / 7\n",
    "\n",
    "print(\"‚úÖ Created time_since_signup in days and weeks\")\n",
    "\n",
    "# 3. Time of day categories\n",
    "def categorize_hour(hour):\n",
    "    if 0 <= hour < 6:\n",
    "        return 'night'\n",
    "    elif 6 <= hour < 12:\n",
    "        return 'morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'afternoon'\n",
    "    else:\n",
    "        return 'evening'\n",
    "\n",
    "df_ecom_fe['time_of_day'] = df_ecom_fe['purchase_hour'].apply(categorize_hour)\n",
    "\n",
    "# 4. Weekend vs weekday\n",
    "df_ecom_fe['is_weekend'] = df_ecom_fe['purchase_day'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "print(\"‚úÖ Created time_of_day categories and is_weekend flag\")\n",
    "\n",
    "print(\"\\nüë§ USER BEHAVIOR FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 5. User transaction frequency (velocity)\n",
    "# Calculate number of transactions per user\n",
    "user_txn_counts = df_ecom_fe['user_id'].value_counts().reset_index()\n",
    "user_txn_counts.columns = ['user_id', 'user_total_transactions']\n",
    "df_ecom_fe = df_ecom_fe.merge(user_txn_counts, on='user_id', how='left')\n",
    "\n",
    "# Calculate transaction frequency in different time windows\n",
    "# Sort by user and purchase time\n",
    "df_ecom_fe = df_ecom_fe.sort_values(['user_id', 'purchase_time'])\n",
    "\n",
    "# Time since last transaction for same user\n",
    "df_ecom_fe['time_since_last_txn_hours'] = df_ecom_fe.groupby('user_id')['purchase_time'].diff().dt.total_seconds() / 3600\n",
    "\n",
    "# Fill first transaction with a large value (e.g., 30 days)\n",
    "df_ecom_fe['time_since_last_txn_hours'] = df_ecom_fe['time_since_last_txn_hours'].fillna(30*24)\n",
    "\n",
    "print(\"‚úÖ Created user transaction features: user_total_transactions, time_since_last_txn_hours\")\n",
    "\n",
    "# 6. Device usage patterns\n",
    "device_usage_counts = df_ecom_fe['device_id'].value_counts().reset_index()\n",
    "device_usage_counts.columns = ['device_id', 'device_usage_count']\n",
    "df_ecom_fe = df_ecom_fe.merge(device_usage_counts, on='device_id', how='left')\n",
    "\n",
    "print(\"‚úÖ Created device usage feature: device_usage_count\")\n",
    "\n",
    "print(\"\\nüåç GEOGRAPHICAL & RISK FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 7. Country risk encoding (fraud rate by country)\n",
    "if 'country' in df_ecom_fe.columns:\n",
    "    # Calculate fraud rate by country\n",
    "    country_fraud_rate = df_ecom_fe.groupby('country')['class'].mean().reset_index()\n",
    "    country_fraud_rate.columns = ['country', 'country_fraud_rate']\n",
    "    \n",
    "    df_ecom_fe = df_ecom_fe.merge(country_fraud_rate, on='country', how='left')\n",
    "    \n",
    "    # Create risk categories\n",
    "    def categorize_country_risk(rate):\n",
    "        if rate > 0.15:\n",
    "            return 'high_risk'\n",
    "        elif rate > 0.05:\n",
    "            return 'medium_risk'\n",
    "        else:\n",
    "            return 'low_risk'\n",
    "    \n",
    "    df_ecom_fe['country_risk_category'] = df_ecom_fe['country_fraud_rate'].apply(categorize_country_risk)\n",
    "    \n",
    "    print(\"‚úÖ Created country risk features: country_fraud_rate, country_risk_category\")\n",
    "\n",
    "print(\"\\nüí∞ PURCHASE BEHAVIOR FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 8. Purchase value features\n",
    "df_ecom_fe['purchase_value_log'] = np.log1p(df_ecom_fe['purchase_value'])\n",
    "df_ecom_fe['purchase_value_sqrt'] = np.sqrt(df_ecom_fe['purchase_value'])\n",
    "\n",
    "# Purchase value categories\n",
    "def categorize_purchase_value(value):\n",
    "    if value < 20:\n",
    "        return 'low'\n",
    "    elif value < 50:\n",
    "        return 'medium'\n",
    "    elif value < 100:\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'very_high'\n",
    "\n",
    "df_ecom_fe['purchase_value_category'] = df_ecom_fe['purchase_value'].apply(categorize_purchase_value)\n",
    "\n",
    "print(\"‚úÖ Created purchase value features: log transform, sqrt transform, categories\")\n",
    "\n",
    "# 9. Age group features\n",
    "def categorize_age(age):\n",
    "    if age < 25:\n",
    "        return 'young'\n",
    "    elif age < 40:\n",
    "        return 'adult'\n",
    "    elif age < 60:\n",
    "        return 'middle_aged'\n",
    "    else:\n",
    "        return 'senior'\n",
    "\n",
    "df_ecom_fe['age_group'] = df_ecom_fe['age'].apply(categorize_age)\n",
    "\n",
    "print(\"‚úÖ Created age_group feature\")\n",
    "\n",
    "print(\"\\nüîó INTERACTION FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 10. Interaction between features\n",
    "# New user flag (transactions within 24 hours of signup)\n",
    "df_ecom_fe['is_new_user'] = df_ecom_fe['time_since_signup_hours'].apply(lambda x: 1 if x < 24 else 0)\n",
    "\n",
    "# High value new user\n",
    "df_ecom_fe['high_value_new_user'] = ((df_ecom_fe['is_new_user'] == 1) & \n",
    "                                     (df_ecom_fe['purchase_value_category'] == 'high')).astype(int)\n",
    "\n",
    "# Unusual hour purchase\n",
    "df_ecom_fe['unusual_hour_purchase'] = df_ecom_fe['purchase_hour'].apply(lambda x: 1 if x < 6 or x > 22 else 0)\n",
    "\n",
    "print(\"‚úÖ Created interaction features: is_new_user, high_value_new_user, unusual_hour_purchase\")\n",
    "\n",
    "# Display feature engineering summary\n",
    "print(f\"\\nüéØ FEATURE ENGINEERING SUMMARY:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Original features: {len(df_ecom.columns)}\")\n",
    "print(f\"After feature engineering: {len(df_ecom_fe.columns)}\")\n",
    "print(f\"New features added: {len(df_ecom_fe.columns) - len(df_ecom.columns)}\")\n",
    "\n",
    "print(f\"\\nüìä New columns created:\")\n",
    "new_columns = set(df_ecom_fe.columns) - set(df_ecom.columns)\n",
    "for i, col in enumerate(sorted(new_columns), 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\n‚úÖ E-commerce feature engineering completed!\")\n",
    "print(f\"   Final shape: {df_ecom_fe.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2d7db",
   "metadata": {},
   "source": [
    "## Data Transformation - E-commerce Data\n",
    "\n",
    "Now we need to:\n",
    "1. Encode categorical features (One-Hot Encoding)\n",
    "2. Scale numerical features\n",
    "3. Prepare the dataset for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42bc5abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATA TRANSFORMATION - E-COMMERCE DATA\n",
      "==================================================\n",
      "Dataset shape before transformation: (129146, 39)\n",
      "\n",
      "üîç COLUMN ANALYSIS:\n",
      "------------------------------\n",
      "Categorical features (16):\n",
      "  ‚Ä¢ source                    -   3 unique values\n",
      "  ‚Ä¢ browser                   -   5 unique values\n",
      "  ‚Ä¢ sex                       -   2 unique values\n",
      "  ‚Ä¢ class                     -   2 unique values\n",
      "  ‚Ä¢ country                   - 181 unique values\n",
      "  ‚Ä¢ purchase_day              -   7 unique values\n",
      "  ‚Ä¢ purchase_month            -  12 unique values\n",
      "  ‚Ä¢ signup_day                -   7 unique values\n",
      "  ‚Ä¢ is_weekend                -   2 unique values\n",
      "  ‚Ä¢ user_total_transactions   -   1 unique values\n",
      "  ... and 6 more\n",
      "\n",
      "Numerical features (18):\n",
      "  ‚Ä¢ user_id\n",
      "  ‚Ä¢ purchase_value\n",
      "  ‚Ä¢ age\n",
      "  ‚Ä¢ ip_address\n",
      "  ‚Ä¢ ip_address_int\n",
      "  ‚Ä¢ lower_bound_ip_address\n",
      "  ‚Ä¢ upper_bound_ip_address\n",
      "  ‚Ä¢ purchase_hour\n",
      "  ‚Ä¢ time_since_signup_hours\n",
      "  ‚Ä¢ purchase_day_of_month\n",
      "  ... and 8 more\n",
      "\n",
      "ID columns (1): ['device_id']\n",
      "Datetime columns (2): ['signup_time', 'purchase_time']\n",
      "\n",
      "üéØ TARGET VARIABLE:\n",
      "  ‚Ä¢ Target column: 'class'\n",
      "  ‚Ä¢ Distribution: {0: 116878, 1: 12268}\n",
      "\n",
      "üìä ONE-HOT ENCODING:\n",
      "------------------------------\n",
      "Categorical columns to encode (14):\n",
      "  ‚Ä¢ source                    -  3 unique values\n",
      "  ‚Ä¢ browser                   -  5 unique values\n",
      "  ‚Ä¢ sex                       -  2 unique values\n",
      "  ‚Ä¢ purchase_day              -  7 unique values\n",
      "  ‚Ä¢ purchase_month            - 12 unique values\n",
      "  ‚Ä¢ signup_day                -  7 unique values\n",
      "  ‚Ä¢ is_weekend                -  2 unique values\n",
      "  ‚Ä¢ user_total_transactions   -  1 unique values\n",
      "  ‚Ä¢ country_risk_category     -  3 unique values\n",
      "  ‚Ä¢ purchase_value_category   -  4 unique values\n",
      "  ‚Ä¢ age_group                 -  4 unique values\n",
      "  ‚Ä¢ is_new_user               -  2 unique values\n",
      "  ‚Ä¢ high_value_new_user       -  2 unique values\n",
      "  ‚Ä¢ unusual_hour_purchase     -  2 unique values\n",
      "‚úÖ One-Hot Encoding completed\n",
      "   Features before encoding: 39\n",
      "   Features after encoding: 67\n",
      "\n",
      "üìä FREQUENCY ENCODING FOR HIGH-CARDINALITY FEATURES:\n",
      "------------------------------\n",
      "  ‚Ä¢ country                   - Frequency encoded and dropped\n",
      "\n",
      "üóëÔ∏è  DROPPING NON-FEATURE COLUMNS:\n",
      "------------------------------\n",
      "Dropping columns (7):\n",
      "  ‚Ä¢ device_id\n",
      "  ‚Ä¢ signup_time\n",
      "  ‚Ä¢ purchase_time\n",
      "  ‚Ä¢ ip_address\n",
      "  ‚Ä¢ ip_address_int\n",
      "  ‚Ä¢ lower_bound_ip_address\n",
      "  ‚Ä¢ upper_bound_ip_address\n",
      "\n",
      "üîß HANDLING MISSING VALUES:\n",
      "------------------------------\n",
      "Missing values before handling: 0\n",
      "Missing values after handling: 0\n",
      "\n",
      "‚úÖ Data transformation completed!\n",
      "   Final dataset shape: (129146, 60)\n",
      "   Total features: 60\n",
      "   Target column: 'class' (preserved)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"DATA TRANSFORMATION - E-COMMERCE DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Make a copy for transformation\n",
    "df_ecom_transformed = df_ecom_fe.copy()\n",
    "\n",
    "print(f\"Dataset shape before transformation: {df_ecom_transformed.shape}\")\n",
    "\n",
    "# Identify column types\n",
    "print(\"\\nüîç COLUMN ANALYSIS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Separate columns by type\n",
    "categorical_cols = []\n",
    "numerical_cols = []\n",
    "datetime_cols = []\n",
    "id_cols = []\n",
    "text_cols = []\n",
    "\n",
    "for col in df_ecom_transformed.columns:\n",
    "    dtype = df_ecom_transformed[col].dtype\n",
    "    \n",
    "    if dtype == 'object' or df_ecom_transformed[col].nunique() < 20:\n",
    "        if 'time' not in col.lower() and 'id' not in col.lower():\n",
    "            categorical_cols.append(col)\n",
    "        elif 'id' in col.lower():\n",
    "            id_cols.append(col)\n",
    "    elif 'int' in str(dtype) or 'float' in str(dtype):\n",
    "        if 'class' not in col.lower():\n",
    "            numerical_cols.append(col)\n",
    "    elif 'datetime' in str(dtype):\n",
    "        datetime_cols.append(col)\n",
    "\n",
    "print(f\"Categorical features ({len(categorical_cols)}):\")\n",
    "for col in categorical_cols[:10]:  # Show first 10\n",
    "    unique_vals = df_ecom_transformed[col].nunique()\n",
    "    print(f\"  ‚Ä¢ {col:25s} - {unique_vals:3d} unique values\")\n",
    "if len(categorical_cols) > 10:\n",
    "    print(f\"  ... and {len(categorical_cols) - 10} more\")\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_cols)}):\")\n",
    "for col in numerical_cols[:10]:  # Show first 10\n",
    "    print(f\"  ‚Ä¢ {col}\")\n",
    "if len(numerical_cols) > 10:\n",
    "    print(f\"  ... and {len(numerical_cols) - 10} more\")\n",
    "\n",
    "print(f\"\\nID columns ({len(id_cols)}): {id_cols}\")\n",
    "print(f\"Datetime columns ({len(datetime_cols)}): {datetime_cols}\")\n",
    "\n",
    "print(\"\\nüéØ TARGET VARIABLE:\")\n",
    "print(f\"  ‚Ä¢ Target column: 'class'\")\n",
    "print(f\"  ‚Ä¢ Distribution: {df_ecom_transformed['class'].value_counts().to_dict()}\")\n",
    "\n",
    "# 1. Handle categorical features - One-Hot Encoding\n",
    "print(\"\\nüìä ONE-HOT ENCODING:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Select categorical columns with reasonable cardinality (avoid high cardinality)\n",
    "categorical_to_encode = [col for col in categorical_cols \n",
    "                        if df_ecom_transformed[col].nunique() <= 15 and col != 'class']\n",
    "\n",
    "print(f\"Categorical columns to encode ({len(categorical_to_encode)}):\")\n",
    "for col in categorical_to_encode:\n",
    "    unique_vals = df_ecom_transformed[col].nunique()\n",
    "    print(f\"  ‚Ä¢ {col:25s} - {unique_vals:2d} unique values\")\n",
    "\n",
    "# Perform One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df_ecom_transformed, \n",
    "                           columns=categorical_to_encode,\n",
    "                           prefix=categorical_to_encode,\n",
    "                           drop_first=True)  # Avoid dummy variable trap\n",
    "\n",
    "print(f\"‚úÖ One-Hot Encoding completed\")\n",
    "print(f\"   Features before encoding: {len(df_ecom_transformed.columns)}\")\n",
    "print(f\"   Features after encoding: {len(df_encoded.columns)}\")\n",
    "\n",
    "# 2. Handle high cardinality categorical features - Frequency Encoding\n",
    "print(\"\\nüìä FREQUENCY ENCODING FOR HIGH-CARDINALITY FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "high_cardinality_cols = [col for col in categorical_cols \n",
    "                        if col not in categorical_to_encode and col != 'class']\n",
    "\n",
    "for col in high_cardinality_cols:\n",
    "    if col in df_encoded.columns:\n",
    "        # Frequency encoding\n",
    "        freq_encoding = df_encoded[col].value_counts(normalize=True)\n",
    "        df_encoded[f'{col}_freq_encoded'] = df_encoded[col].map(freq_encoding)\n",
    "        # Drop original column\n",
    "        df_encoded = df_encoded.drop(columns=[col])\n",
    "        print(f\"  ‚Ä¢ {col:25s} - Frequency encoded and dropped\")\n",
    "\n",
    "# 3. Drop ID and datetime columns (not useful for modeling)\n",
    "print(\"\\nüóëÔ∏è  DROPPING NON-FEATURE COLUMNS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "columns_to_drop = id_cols + datetime_cols + ['ip_address', 'ip_address_int', \n",
    "                                            'lower_bound_ip_address', 'upper_bound_ip_address']\n",
    "\n",
    "# Only drop columns that exist\n",
    "columns_to_drop = [col for col in columns_to_drop if col in df_encoded.columns]\n",
    "\n",
    "print(f\"Dropping columns ({len(columns_to_drop)}):\")\n",
    "for col in columns_to_drop:\n",
    "    print(f\"  ‚Ä¢ {col}\")\n",
    "\n",
    "df_encoded = df_encoded.drop(columns=columns_to_drop)\n",
    "\n",
    "# 4. Handle missing values\n",
    "print(\"\\nüîß HANDLING MISSING VALUES:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "missing_before = df_encoded.isnull().sum().sum()\n",
    "print(f\"Missing values before handling: {missing_before}\")\n",
    "\n",
    "# Fill numerical missing values with median\n",
    "for col in df_encoded.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    if df_encoded[col].isnull().sum() > 0:\n",
    "        median_val = df_encoded[col].median()\n",
    "        df_encoded[col] = df_encoded[col].fillna(median_val)\n",
    "        print(f\"  ‚Ä¢ {col:30s} - Filled {df_encoded[col].isnull().sum():4d} NaN with median {median_val:.2f}\")\n",
    "\n",
    "missing_after = df_encoded.isnull().sum().sum()\n",
    "print(f\"Missing values after handling: {missing_after}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data transformation completed!\")\n",
    "print(f\"   Final dataset shape: {df_encoded.shape}\")\n",
    "print(f\"   Total features: {len(df_encoded.columns)}\")\n",
    "print(f\"   Target column: 'class' (preserved)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12858532",
   "metadata": {},
   "source": [
    "## Feature Scaling and Class Imbalance Handling\n",
    "\n",
    "Now we need to:\n",
    "1. Scale numerical features using StandardScaler\n",
    "2. Handle class imbalance using SMOTE\n",
    "3. Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad214555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE SCALING AND CLASS IMBALANCE HANDLING\n",
      "============================================================\n",
      "Dataset shape: (129146, 60)\n",
      "Features: 60\n",
      "\n",
      "üîç DATA TYPES CHECK:\n",
      "------------------------------\n",
      "  bool: 42 columns\n",
      "  float64: 8 columns\n",
      "  int64: 6 columns\n",
      "  int32: 3 columns\n",
      "  object: 1 columns\n",
      "\n",
      "‚ö†Ô∏è  Non-numeric columns found: 46\n",
      "  Columns: ['purchase_day_of_month', 'purchase_minute', 'signup_hour', 'time_of_day', 'source_Direct', 'source_SEO', 'browser_FireFox', 'browser_IE', 'browser_Opera', 'browser_Safari', 'sex_M', 'purchase_day_1', 'purchase_day_2', 'purchase_day_3', 'purchase_day_4', 'purchase_day_5', 'purchase_day_6', 'purchase_month_2', 'purchase_month_3', 'purchase_month_4', 'purchase_month_5', 'purchase_month_6', 'purchase_month_7', 'purchase_month_8', 'purchase_month_9', 'purchase_month_10', 'purchase_month_11', 'purchase_month_12', 'signup_day_1', 'signup_day_2', 'signup_day_3', 'signup_day_4', 'signup_day_5', 'signup_day_6', 'is_weekend_1', 'country_risk_category_low_risk', 'country_risk_category_medium_risk', 'purchase_value_category_low', 'purchase_value_category_medium', 'purchase_value_category_very_high', 'age_group_middle_aged', 'age_group_senior', 'age_group_young', 'is_new_user_1', 'high_value_new_user_1', 'unusual_hour_purchase_1']\n",
      "  ‚Ä¢ Converted 'purchase_day_of_month' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_minute' to numeric\n",
      "  ‚Ä¢ Converted 'signup_hour' to numeric\n",
      "  ‚Ä¢ Converted 'time_of_day' to categorical codes\n",
      "  ‚Ä¢ Converted 'source_Direct' to numeric\n",
      "  ‚Ä¢ Converted 'source_SEO' to numeric\n",
      "  ‚Ä¢ Converted 'browser_FireFox' to numeric\n",
      "  ‚Ä¢ Converted 'browser_IE' to numeric\n",
      "  ‚Ä¢ Converted 'browser_Opera' to numeric\n",
      "  ‚Ä¢ Converted 'browser_Safari' to numeric\n",
      "  ‚Ä¢ Converted 'sex_M' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_day_1' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_day_2' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_day_3' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_day_4' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_day_5' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_day_6' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_month_2' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_month_3' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_month_4' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_month_5' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_month_6' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_month_7' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_month_8' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_month_9' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_month_10' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_month_11' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_month_12' to numeric\n",
      "  ‚Ä¢ Converted 'signup_day_1' to numeric\n",
      "  ‚Ä¢ Converted 'signup_day_2' to numeric\n",
      "  ‚Ä¢ Converted 'signup_day_3' to numeric\n",
      "  ‚Ä¢ Converted 'signup_day_4' to numeric\n",
      "  ‚Ä¢ Converted 'signup_day_5' to numeric\n",
      "  ‚Ä¢ Converted 'signup_day_6' to numeric\n",
      "  ‚Ä¢ Converted 'is_weekend_1' to numeric\n",
      "  ‚Ä¢ Converted 'country_risk_category_low_risk' to numeric\n",
      "  ‚Ä¢ Converted 'country_risk_category_medium_risk' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_value_category_low' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_value_category_medium' to numeric\n",
      "  ‚Ä¢ Converted 'purchase_value_category_very_high' to numeric\n",
      "  ‚Ä¢ Converted 'age_group_middle_aged' to numeric\n",
      "  ‚Ä¢ Converted 'age_group_senior' to numeric\n",
      "  ‚Ä¢ Converted 'age_group_young' to numeric\n",
      "  ‚Ä¢ Converted 'is_new_user_1' to numeric\n",
      "  ‚Ä¢ Converted 'high_value_new_user_1' to numeric\n",
      "  ‚Ä¢ Converted 'unusual_hour_purchase_1' to numeric\n",
      "\n",
      "üîç FEATURE-TARGET SEPARATION:\n",
      "  X shape: (129146, 59)\n",
      "  y shape: (129146,)\n",
      "  Target distribution:\n",
      "    Class 0 (Legitimate): 116,878 (90.50%)\n",
      "    Class 1 (Fraud): 12,268 (9.50%)\n",
      "\n",
      "üìä TRAIN-TEST SPLIT (STRATIFIED):\n",
      "----------------------------------------\n",
      "Training set size: 103,316 samples (80.0%)\n",
      "Testing set size: 25,830 samples (20.0%)\n",
      "\n",
      "Training set class distribution:\n",
      "  Class 0: 93,502 (90.50%)\n",
      "  Class 1: 9,814 (9.50%)\n",
      "\n",
      "Testing set class distribution:\n",
      "  Class 0: 23,376 (90.50%)\n",
      "  Class 1: 2,454 (9.50%)\n",
      "\n",
      "üìà FEATURE SCALING (STANDARD SCALER):\n",
      "----------------------------------------\n",
      "Numerical columns to scale: 13\n",
      "Binary columns (not scaled): 0\n",
      "‚úÖ Standard scaling applied to numerical features\n",
      "   Scaled features: user_id, purchase_value, age, purchase_hour, time_since_signup_hours...\n",
      "\n",
      "üìä SCALING STATISTICS (First 5 features):\n",
      "------------------------------\n",
      "user_id                       : Mean=200211.39 ‚Üí     0.00, Std=115221.43 ‚Üí     1.00\n",
      "purchase_value                : Mean=   36.92 ‚Üí    -0.00, Std=   18.31 ‚Üí     1.00\n",
      "age                           : Mean=   33.13 ‚Üí     0.00, Std=    8.63 ‚Üí     1.00\n",
      "purchase_hour                 : Mean=   11.50 ‚Üí    -0.00, Std=    6.91 ‚Üí     1.00\n",
      "time_since_signup_hours       : Mean= 1369.57 ‚Üí    -0.00, Std=  869.09 ‚Üí     1.00\n",
      "\n",
      "‚öñÔ∏è  HANDLING CLASS IMBALANCE WITH SMOTE:\n",
      "----------------------------------------\n",
      "BEFORE SMOTE:\n",
      "  Training set shape: (103316, 59)\n",
      "  Class distribution: Class 0: 93,502, Class 1: 9,814\n",
      "  Imbalance ratio: 9.53:1\n",
      "\n",
      "AFTER SMOTE:\n",
      "  Resampled training set shape: (140253, 59)\n",
      "  Class distribution: Class 0: 93,502, Class 1: 46,751\n",
      "  New imbalance ratio: 2.00:1\n",
      "  Fraud samples increased by: 376%\n",
      "\n",
      "üíæ SAVING PROCESSED DATASETS:\n",
      "----------------------------------------\n",
      "‚úÖ Training data saved to: ../data/processed/ecommerce_train_processed.csv\n",
      "   Shape: (140253, 60), Size: 140,253 samples\n",
      "   Class distribution: 0=93,502, 1=46,751\n",
      "\n",
      "‚úÖ Testing data saved to: ../data/processed/ecommerce_test_processed.csv\n",
      "   Shape: (25830, 60), Size: 25,830 samples\n",
      "   Class distribution: 0=23,376, 1=2,454\n",
      "\n",
      "‚úÖ Scaler saved to: ../models/ecommerce_scaler.pkl\n",
      "\n",
      "üéØ E-COMMERCE DATA PREPARATION COMPLETED!\n",
      "--------------------------------------------------\n",
      "Original dataset: 129,146 samples, 18 features\n",
      "Final training set: 140,253 samples, 59 features\n",
      "Final testing set: 25,830 samples, 59 features\n",
      "Class imbalance handled: 9.5:1 ‚Üí 2.0:1\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FEATURE SCALING AND CLASS IMBALANCE HANDLING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Make a copy of the transformed data\n",
    "df_final = df_encoded.copy()\n",
    "\n",
    "print(f\"Dataset shape: {df_final.shape}\")\n",
    "print(f\"Features: {len(df_final.columns)}\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nüîç DATA TYPES CHECK:\")\n",
    "print(\"-\" * 30)\n",
    "type_counts = df_final.dtypes.value_counts()\n",
    "for dtype, count in type_counts.items():\n",
    "    print(f\"  {dtype}: {count} columns\")\n",
    "\n",
    "# Find non-numeric columns\n",
    "non_numeric_cols = df_final.select_dtypes(exclude=['int64', 'float64']).columns.tolist()\n",
    "if non_numeric_cols:\n",
    "    print(f\"\\n‚ö†Ô∏è  Non-numeric columns found: {len(non_numeric_cols)}\")\n",
    "    print(f\"  Columns: {non_numeric_cols}\")\n",
    "    \n",
    "    # Convert object columns to categorical codes\n",
    "    for col in non_numeric_cols:\n",
    "        if col != 'class':  # Don't convert target\n",
    "            if df_final[col].dtype == 'object':\n",
    "                # Convert to categorical codes\n",
    "                df_final[col] = df_final[col].astype('category').cat.codes\n",
    "                print(f\"  ‚Ä¢ Converted '{col}' to categorical codes\")\n",
    "            else:\n",
    "                # Try to convert to numeric\n",
    "                df_final[col] = pd.to_numeric(df_final[col], errors='coerce')\n",
    "                print(f\"  ‚Ä¢ Converted '{col}' to numeric\")\n",
    "else:\n",
    "    print(\"‚úÖ All columns are numeric\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_final.drop(columns=['class'])\n",
    "y = df_final['class']\n",
    "\n",
    "print(f\"\\nüîç FEATURE-TARGET SEPARATION:\")\n",
    "print(f\"  X shape: {X.shape}\")\n",
    "print(f\"  y shape: {y.shape}\")\n",
    "print(f\"  Target distribution:\")\n",
    "print(f\"    Class 0 (Legitimate): {(y == 0).sum():,} ({(y == 0).mean()*100:.2f}%)\")\n",
    "print(f\"    Class 1 (Fraud): {(y == 1).sum():,} ({(y == 1).mean()*100:.2f}%)\")\n",
    "\n",
    "# 1. Train-Test Split (Stratified to preserve class distribution)\n",
    "print(\"\\nüìä TRAIN-TEST SPLIT (STRATIFIED):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y,  # Preserve class distribution\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Testing set size: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "train_class_counts = y_train.value_counts()\n",
    "train_class_percent = y_train.value_counts(normalize=True) * 100\n",
    "print(f\"  Class 0: {train_class_counts[0]:,} ({train_class_percent[0]:.2f}%)\")\n",
    "print(f\"  Class 1: {train_class_counts[1]:,} ({train_class_percent[1]:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTesting set class distribution:\")\n",
    "test_class_counts = y_test.value_counts()\n",
    "test_class_percent = y_test.value_counts(normalize=True) * 100\n",
    "print(f\"  Class 0: {test_class_counts[0]:,} ({test_class_percent[0]:.2f}%)\")\n",
    "print(f\"  Class 1: {test_class_counts[1]:,} ({test_class_percent[1]:.2f}%)\")\n",
    "\n",
    "# 2. Feature Scaling\n",
    "print(\"\\nüìà FEATURE SCALING (STANDARD SCALER):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Remove binary columns (0/1) from scaling\n",
    "binary_cols = [col for col in numerical_cols if X_train[col].nunique() == 2]\n",
    "numerical_cols_to_scale = [col for col in numerical_cols if col not in binary_cols]\n",
    "\n",
    "print(f\"Numerical columns to scale: {len(numerical_cols_to_scale)}\")\n",
    "print(f\"Binary columns (not scaled): {len(binary_cols)}\")\n",
    "\n",
    "if numerical_cols_to_scale:\n",
    "    # Initialize scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit on training data only\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    X_train_scaled[numerical_cols_to_scale] = scaler.fit_transform(X_train[numerical_cols_to_scale])\n",
    "    X_test_scaled[numerical_cols_to_scale] = scaler.transform(X_test[numerical_cols_to_scale])\n",
    "    \n",
    "    print(\"‚úÖ Standard scaling applied to numerical features\")\n",
    "    print(f\"   Scaled features: {', '.join(numerical_cols_to_scale[:5])}...\")\n",
    "    \n",
    "    # Show scaling statistics for first few features\n",
    "    print(\"\\nüìä SCALING STATISTICS (First 5 features):\")\n",
    "    print(\"-\" * 30)\n",
    "    for col in numerical_cols_to_scale[:5]:\n",
    "        print(f\"{col:30s}: Mean={X_train[col].mean():8.2f} ‚Üí {X_train_scaled[col].mean():8.2f}, \"\n",
    "              f\"Std={X_train[col].std():8.2f} ‚Üí {X_train_scaled[col].std():8.2f}\")\n",
    "else:\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    print(\"‚ö†Ô∏è  No numerical features to scale (all are binary/categorical)\")\n",
    "\n",
    "# 3. Handle Class Imbalance with SMOTE\n",
    "print(\"\\n‚öñÔ∏è  HANDLING CLASS IMBALANCE WITH SMOTE:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"BEFORE SMOTE:\")\n",
    "print(f\"  Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"  Class distribution: Class 0: {train_class_counts[0]:,}, Class 1: {train_class_counts[1]:,}\")\n",
    "print(f\"  Imbalance ratio: {train_class_counts[0]/train_class_counts[1]:.2f}:1\")\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "try:\n",
    "    smote = SMOTE(random_state=42, sampling_strategy=0.5)  # Balance to 1:2 ratio (fraud:legit)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "    print(\"\\nAFTER SMOTE:\")\n",
    "    print(f\"  Resampled training set shape: {X_train_resampled.shape}\")\n",
    "    print(f\"  Class distribution: Class 0: {(y_train_resampled == 0).sum():,}, \"\n",
    "          f\"Class 1: {(y_train_resampled == 1).sum():,}\")\n",
    "    print(f\"  New imbalance ratio: {(y_train_resampled == 0).sum()/(y_train_resampled == 1).sum():.2f}:1\")\n",
    "    print(f\"  Fraud samples increased by: {((y_train_resampled == 1).sum() - train_class_counts[1])/train_class_counts[1]*100:.0f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  SMOTE failed: {str(e)}\")\n",
    "    print(\"Using RandomUnderSampler as fallback...\")\n",
    "    \n",
    "    # Use RandomUnderSampler as alternative\n",
    "    rus = RandomUnderSampler(random_state=42, sampling_strategy=0.5)\n",
    "    X_train_resampled, y_train_resampled = rus.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "    print(\"\\nAFTER RANDOM UNDER SAMPLING:\")\n",
    "    print(f\"  Resampled training set shape: {X_train_resampled.shape}\")\n",
    "    print(f\"  Class distribution: Class 0: {(y_train_resampled == 0).sum():,}, \"\n",
    "          f\"Class 1: {(y_train_resampled == 1).sum():,}\")\n",
    "    print(f\"  New imbalance ratio: {(y_train_resampled == 0).sum()/(y_train_resampled == 1).sum():.2f}:1\")\n",
    "\n",
    "# 4. Save the processed datasets\n",
    "print(\"\\nüíæ SAVING PROCESSED DATASETS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create DataFrames for saving\n",
    "train_df = pd.DataFrame(X_train_resampled, columns=X_train_scaled.columns)\n",
    "train_df['class'] = y_train_resampled\n",
    "\n",
    "test_df = pd.DataFrame(X_test_scaled, columns=X_test_scaled.columns)\n",
    "test_df['class'] = y_test\n",
    "\n",
    "# Save to CSV\n",
    "train_path = '../data/processed/ecommerce_train_processed.csv'\n",
    "test_path = '../data/processed/ecommerce_test_processed.csv'\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Training data saved to: {train_path}\")\n",
    "print(f\"   Shape: {train_df.shape}, Size: {len(train_df):,} samples\")\n",
    "print(f\"   Class distribution: 0={(train_df['class'] == 0).sum():,}, 1={(train_df['class'] == 1).sum():,}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Testing data saved to: {test_path}\")\n",
    "print(f\"   Shape: {test_df.shape}, Size: {len(test_df):,} samples\")\n",
    "print(f\"   Class distribution: 0={(test_df['class'] == 0).sum():,}, 1={(test_df['class'] == 1).sum():,}\")\n",
    "\n",
    "# Save the scaler for future use\n",
    "import joblib\n",
    "scaler_path = '../models/ecommerce_scaler.pkl'\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"\\n‚úÖ Scaler saved to: {scaler_path}\")\n",
    "\n",
    "print(f\"\\nüéØ E-COMMERCE DATA PREPARATION COMPLETED!\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Original dataset: {df_ecom.shape[0]:,} samples, {df_ecom.shape[1]} features\")\n",
    "print(f\"Final training set: {len(train_df):,} samples, {len(train_df.columns)-1} features\")\n",
    "print(f\"Final testing set: {len(test_df):,} samples, {len(test_df.columns)-1} features\")\n",
    "print(f\"Class imbalance handled: {train_class_counts[0]/train_class_counts[1]:.1f}:1 ‚Üí {(train_df['class'] == 0).sum()/(train_df['class'] == 1).sum():.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a804fc1f",
   "metadata": {},
   "source": [
    "## Feature Engineering - Credit Card Data\n",
    "\n",
    "Now let's process the credit card data. Since PCA features are already transformed, we'll focus on:\n",
    "1. Time feature engineering\n",
    "2. Amount feature transformation\n",
    "3. Creating interaction features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bee4f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "FEATURE ENGINEERING - CREDIT CARD DATA\n",
      "==================================================\n",
      "Original shape: (283726, 31)\n",
      "Target column: 'Class'\n",
      "Fraud rate: 0.166710%\n",
      "\n",
      "üìä CURRENT FEATURES:\n",
      "------------------------------\n",
      "‚Ä¢ Time: Elapsed seconds since first transaction\n",
      "‚Ä¢ V1-V28: PCA-transformed features (anonymized)\n",
      "‚Ä¢ Amount: Transaction amount in dollars\n",
      "‚Ä¢ Class: Target variable (0=legit, 1=fraud)\n",
      "\n",
      "üéØ FEATURE ENGINEERING TASKS:\n",
      "------------------------------\n",
      "1. Transform Time feature into meaningful units\n",
      "2. Handle Amount feature (log transform, binning)\n",
      "3. Create interaction features\n",
      "4. Add statistical features\n",
      "\n",
      "üìÖ TIME FEATURE ENGINEERING:\n",
      "------------------------------\n",
      "‚úÖ Created time features: Time_hours, Time_sin, Time_cos, Time_diff\n",
      "\n",
      "üí∞ AMOUNT FEATURE ENGINEERING:\n",
      "------------------------------\n",
      "‚úÖ Created amount features: log transform, sqrt transform, z-score, categories\n",
      "\n",
      "üìà STATISTICAL FEATURES:\n",
      "------------------------------\n",
      "‚úÖ Created statistical features: rolling mean, rolling std, high amount flag\n",
      "\n",
      "üîó INTERACTION FEATURES:\n",
      "------------------------------\n",
      "‚úÖ Created interaction features: squared terms, amount-PCA interactions\n",
      "\n",
      "üö® ANOMALY SCORE FEATURES:\n",
      "------------------------------\n",
      "‚úÖ Created anomaly score features: individual z-scores, combined anomaly score\n",
      "\n",
      "üéØ FEATURE ENGINEERING SUMMARY:\n",
      "----------------------------------------\n",
      "Original features: 31\n",
      "After feature engineering: 52\n",
      "New features added: 21\n",
      "\n",
      "üìä New columns created:\n",
      "   1. Amount_V10_interaction\n",
      "   2. Amount_V14_interaction\n",
      "   3. Amount_category\n",
      "   4. Amount_high_relative\n",
      "   5. Amount_log\n",
      "   6. Amount_rolling_mean\n",
      "   7. Amount_rolling_std\n",
      "   8. Amount_sqrt\n",
      "   9. Amount_zscore\n",
      "  10. Time_cos\n",
      "  11. Time_diff\n",
      "  12. Time_hours\n",
      "  13. Time_sin\n",
      "  14. V10_squared\n",
      "  15. V10_zscore\n",
      "  16. V12_zscore\n",
      "  17. V14_squared\n",
      "  18. V14_zscore\n",
      "  19. V16_zscore\n",
      "  20. V17_zscore\n",
      "  21. combined_anomaly_score\n",
      "\n",
      "‚úÖ Credit card feature engineering completed!\n",
      "   Final shape: (283726, 52)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"FEATURE ENGINEERING - CREDIT CARD DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Make a copy for feature engineering\n",
    "df_cc_fe = df_cc.copy()\n",
    "\n",
    "print(f\"Original shape: {df_cc_fe.shape}\")\n",
    "print(f\"Target column: 'Class'\")\n",
    "print(f\"Fraud rate: {df_cc_fe['Class'].mean()*100:.6f}%\")\n",
    "\n",
    "print(\"\\nüìä CURRENT FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"‚Ä¢ Time: Elapsed seconds since first transaction\")\n",
    "print(f\"‚Ä¢ V1-V28: PCA-transformed features (anonymized)\")\n",
    "print(f\"‚Ä¢ Amount: Transaction amount in dollars\")\n",
    "print(f\"‚Ä¢ Class: Target variable (0=legit, 1=fraud)\")\n",
    "\n",
    "print(\"\\nüéØ FEATURE ENGINEERING TASKS:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"1. Transform Time feature into meaningful units\")\n",
    "print(\"2. Handle Amount feature (log transform, binning)\")\n",
    "print(\"3. Create interaction features\")\n",
    "print(\"4. Add statistical features\")\n",
    "\n",
    "# 1. Time Feature Engineering\n",
    "print(\"\\nüìÖ TIME FEATURE ENGINEERING:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Convert seconds to hours\n",
    "df_cc_fe['Time_hours'] = df_cc_fe['Time'] / 3600\n",
    "\n",
    "# Extract hour of day (assuming data starts at some arbitrary time)\n",
    "# Create cyclical time features\n",
    "df_cc_fe['Time_sin'] = np.sin(2 * np.pi * df_cc_fe['Time_hours'] / 24)\n",
    "df_cc_fe['Time_cos'] = np.cos(2 * np.pi * df_cc_fe['Time_hours'] / 24)\n",
    "\n",
    "# Time since last transaction (approximation)\n",
    "df_cc_fe_sorted = df_cc_fe.sort_values('Time')\n",
    "df_cc_fe_sorted['Time_diff'] = df_cc_fe_sorted['Time'].diff()\n",
    "df_cc_fe = df_cc_fe_sorted.copy()\n",
    "\n",
    "print(\"‚úÖ Created time features: Time_hours, Time_sin, Time_cos, Time_diff\")\n",
    "\n",
    "# 2. Amount Feature Engineering\n",
    "print(\"\\nüí∞ AMOUNT FEATURE ENGINEERING:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Log transform to handle skewness\n",
    "df_cc_fe['Amount_log'] = np.log1p(df_cc_fe['Amount'])\n",
    "\n",
    "# Square root transform\n",
    "df_cc_fe['Amount_sqrt'] = np.sqrt(df_cc_fe['Amount'])\n",
    "\n",
    "# Standardized amount\n",
    "amount_mean = df_cc_fe['Amount'].mean()\n",
    "amount_std = df_cc_fe['Amount'].std()\n",
    "df_cc_fe['Amount_zscore'] = (df_cc_fe['Amount'] - amount_mean) / amount_std\n",
    "\n",
    "# Amount categories (bins)\n",
    "bins = [0, 10, 50, 100, 500, 1000, 5000, 10000, 50000]\n",
    "labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High', 'Premium', 'Luxury', 'Extreme']\n",
    "df_cc_fe['Amount_category'] = pd.cut(df_cc_fe['Amount'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "print(\"‚úÖ Created amount features: log transform, sqrt transform, z-score, categories\")\n",
    "\n",
    "# 3. Statistical Features\n",
    "print(\"\\nüìà STATISTICAL FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Calculate rolling statistics (using recent transactions)\n",
    "window_size = 100  # Last 100 transactions\n",
    "\n",
    "# Rolling mean and std of amount\n",
    "df_cc_fe['Amount_rolling_mean'] = df_cc_fe['Amount'].rolling(window=window_size, min_periods=1).mean()\n",
    "df_cc_fe['Amount_rolling_std'] = df_cc_fe['Amount'].rolling(window=window_size, min_periods=1).std()\n",
    "\n",
    "# Flag for high amount relative to recent history\n",
    "df_cc_fe['Amount_high_relative'] = (df_cc_fe['Amount'] > \n",
    "                                   (df_cc_fe['Amount_rolling_mean'] + 2 * df_cc_fe['Amount_rolling_std'])).astype(int)\n",
    "\n",
    "print(\"‚úÖ Created statistical features: rolling mean, rolling std, high amount flag\")\n",
    "\n",
    "# 4. PCA Feature Interactions\n",
    "print(\"\\nüîó INTERACTION FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create interaction between top correlated features with fraud\n",
    "# Based on EDA, V14, V10, V12, V16, V17 showed high correlation with fraud\n",
    "\n",
    "# Square of important features (capture non-linear relationships)\n",
    "df_cc_fe['V14_squared'] = df_cc_fe['V14'] ** 2\n",
    "df_cc_fe['V10_squared'] = df_cc_fe['V10'] ** 2\n",
    "\n",
    "# Interaction between amount and PCA features\n",
    "df_cc_fe['Amount_V14_interaction'] = df_cc_fe['Amount'] * df_cc_fe['V14']\n",
    "df_cc_fe['Amount_V10_interaction'] = df_cc_fe['Amount'] * df_cc_fe['V10']\n",
    "\n",
    "print(\"‚úÖ Created interaction features: squared terms, amount-PCA interactions\")\n",
    "\n",
    "# 5. Anomaly Score Features\n",
    "print(\"\\nüö® ANOMALY SCORE FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Calculate z-scores for top PCA features\n",
    "for feature in ['V14', 'V10', 'V12', 'V16', 'V17']:\n",
    "    mean_val = df_cc_fe[feature].mean()\n",
    "    std_val = df_cc_fe[feature].std()\n",
    "    df_cc_fe[f'{feature}_zscore'] = (df_cc_fe[feature] - mean_val) / std_val\n",
    "\n",
    "# Combined anomaly score\n",
    "zscore_cols = [f'{f}_zscore' for f in ['V14', 'V10', 'V12', 'V16', 'V17']]\n",
    "df_cc_fe['combined_anomaly_score'] = df_cc_fe[zscore_cols].abs().mean(axis=1)\n",
    "\n",
    "print(\"‚úÖ Created anomaly score features: individual z-scores, combined anomaly score\")\n",
    "\n",
    "# Display feature engineering summary\n",
    "print(f\"\\nüéØ FEATURE ENGINEERING SUMMARY:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Original features: {len(df_cc.columns)}\")\n",
    "print(f\"After feature engineering: {len(df_cc_fe.columns)}\")\n",
    "print(f\"New features added: {len(df_cc_fe.columns) - len(df_cc.columns)}\")\n",
    "\n",
    "print(f\"\\nüìä New columns created:\")\n",
    "new_columns = set(df_cc_fe.columns) - set(df_cc.columns)\n",
    "for i, col in enumerate(sorted(new_columns), 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Credit card feature engineering completed!\")\n",
    "print(f\"   Final shape: {df_cc_fe.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9118dc",
   "metadata": {},
   "source": [
    "## Data Transformation - Credit Card Data\n",
    "\n",
    "Now we need to process the credit card data:\n",
    "1. Handle categorical features (Amount_category)\n",
    "2. Scale numerical features\n",
    "3. Handle extreme class imbalance\n",
    "4. Split into train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "621ec395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DATA TRANSFORMATION - CREDIT CARD DATA\n",
      "==================================================\n",
      "Dataset shape before transformation: (283726, 52)\n",
      "Target column: 'Class'\n",
      "Fraud rate: 0.166710%\n",
      "\n",
      "üîç DATA TYPES CHECK:\n",
      "------------------------------\n",
      "  float64: 49 columns\n",
      "  int64: 2 columns\n",
      "  category: 1 columns\n",
      "\n",
      "üìä HANDLING CATEGORICAL FEATURES:\n",
      "------------------------------\n",
      "Categorical columns found: 1\n",
      "  ‚Ä¢ Amount_category           -  8 unique values\n",
      "‚úÖ One-Hot Encoding applied to categorical features\n",
      "\n",
      "Shape after encoding: (283726, 58)\n",
      "\n",
      "üîç FEATURE-TARGET SEPARATION:\n",
      "  X shape: (283726, 57)\n",
      "  y shape: (283726,)\n",
      "  Target distribution:\n",
      "    Class 0 (Legitimate): 283,253 (99.833290%)\n",
      "    Class 1 (Fraud): 473 (0.166710%)\n",
      "  Imbalance ratio: 598.8:1\n",
      "\n",
      "üìä TRAIN-TEST SPLIT (STRATIFIED):\n",
      "----------------------------------------\n",
      "Training set size: 226,980 samples (80.0%)\n",
      "Testing set size: 56,746 samples (20.0%)\n",
      "\n",
      "Training set class distribution:\n",
      "  Class 0: 226,602 (99.833466%)\n",
      "  Class 1: 378 (0.166534%)\n",
      "\n",
      "Testing set class distribution:\n",
      "  Class 0: 56,651 (99.832587%)\n",
      "  Class 1: 95 (0.167413%)\n",
      "\n",
      "üìà FEATURE SCALING (STANDARD SCALER):\n",
      "----------------------------------------\n",
      "PCA features (already standardized): 35\n",
      "New features to scale: 13\n",
      "  Features: Time_hours, Time_sin, Time_cos, Time_diff, Amount_log, Amount_sqrt, Amount_zscore, Amount_rolling_mean, Amount_rolling_std, Amount_high_relative...\n",
      "‚úÖ Standard scaling applied to new features\n",
      "\n",
      "üìä SCALING STATISTICS (First 5 new features):\n",
      "------------------------------\n",
      "Time_hours                    : Mean=   26.36 ‚Üí    -0.00, Std=   13.19 ‚Üí     1.00\n",
      "Time_sin                      : Mean=   -0.27 ‚Üí     0.00, Std=    0.63 ‚Üí     1.00\n",
      "Time_cos                      : Mean=   -0.14 ‚Üí    -0.00, Std=    0.72 ‚Üí     1.00\n",
      "Time_diff                     : Mean=    0.61 ‚Üí     0.00, Std=    1.05 ‚Üí     1.00\n",
      "Amount_log                    : Mean=    3.16 ‚Üí    -0.00, Std=    1.66 ‚Üí     1.00\n",
      "\n",
      "‚öñÔ∏è  HANDLING EXTREME CLASS IMBALANCE:\n",
      "----------------------------------------\n",
      "BEFORE RESAMPLING:\n",
      "  Training set shape: (226980, 57)\n",
      "  Class distribution: Class 0: 226,602, Class 1: 378\n",
      "  Imbalance ratio: 599.5:1\n",
      "\n",
      "‚ö†Ô∏è  SMOTE failed: Input X contains NaN.\n",
      "SMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "Using RandomUnderSampler as fallback...\n",
      "\n",
      "AFTER RANDOM UNDER SAMPLING:\n",
      "  Resampled training set shape: (38178, 57)\n",
      "  Class distribution: Class 0: 37,800, Class 1: 378\n",
      "  New imbalance ratio: 100.0:1\n",
      "\n",
      "üíæ SAVING PROCESSED DATASETS:\n",
      "----------------------------------------\n",
      "‚úÖ Training data saved to: ../data/processed/creditcard_train_processed.csv\n",
      "   Shape: (38178, 58), Size: 38,178 samples\n",
      "   Class distribution: 0=37,800, 1=378\n",
      "\n",
      "‚úÖ Testing data saved to: ../data/processed/creditcard_test_processed.csv\n",
      "   Shape: (56746, 58), Size: 56,746 samples\n",
      "   Class distribution: 0=56,651, 1=95\n",
      "\n",
      "‚úÖ Scaler saved to: ../models/creditcard_scaler.pkl\n",
      "\n",
      "üéØ CREDIT CARD DATA PREPARATION COMPLETED!\n",
      "--------------------------------------------------\n",
      "Original dataset: 283,726 samples, 31 features\n",
      "Final training set: 38,178 samples, 57 features\n",
      "Final testing set: 56,746 samples, 57 features\n",
      "Class imbalance handled: 599:1 ‚Üí 100:1\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"DATA TRANSFORMATION - CREDIT CARD DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Make a copy for transformation\n",
    "df_cc_transformed = df_cc_fe.copy()\n",
    "\n",
    "print(f\"Dataset shape before transformation: {df_cc_transformed.shape}\")\n",
    "print(f\"Target column: 'Class'\")\n",
    "print(f\"Fraud rate: {df_cc_transformed['Class'].mean()*100:.6f}%\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nüîç DATA TYPES CHECK:\")\n",
    "print(\"-\" * 30)\n",
    "type_counts = df_cc_transformed.dtypes.value_counts()\n",
    "for dtype, count in type_counts.items():\n",
    "    print(f\"  {dtype}: {count} columns\")\n",
    "\n",
    "# 1. Handle categorical features\n",
    "print(\"\\nüìä HANDLING CATEGORICAL FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check for categorical columns\n",
    "categorical_cols = df_cc_transformed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"Categorical columns found: {len(categorical_cols)}\")\n",
    "if categorical_cols:\n",
    "    for col in categorical_cols:\n",
    "        unique_vals = df_cc_transformed[col].nunique()\n",
    "        print(f\"  ‚Ä¢ {col:25s} - {unique_vals:2d} unique values\")\n",
    "    \n",
    "    # Convert categorical to one-hot encoding\n",
    "    df_cc_transformed = pd.get_dummies(df_cc_transformed, \n",
    "                                       columns=categorical_cols,\n",
    "                                       drop_first=True)  # Avoid dummy variable trap\n",
    "    print(f\"‚úÖ One-Hot Encoding applied to categorical features\")\n",
    "else:\n",
    "    print(\"‚úÖ No categorical features found\")\n",
    "\n",
    "print(f\"\\nShape after encoding: {df_cc_transformed.shape}\")\n",
    "\n",
    "# 2. Separate features and target\n",
    "X_cc = df_cc_transformed.drop(columns=['Class'])\n",
    "y_cc = df_cc_transformed['Class']\n",
    "\n",
    "print(f\"\\nüîç FEATURE-TARGET SEPARATION:\")\n",
    "print(f\"  X shape: {X_cc.shape}\")\n",
    "print(f\"  y shape: {y_cc.shape}\")\n",
    "print(f\"  Target distribution:\")\n",
    "print(f\"    Class 0 (Legitimate): {(y_cc == 0).sum():,} ({(y_cc == 0).mean()*100:.6f}%)\")\n",
    "print(f\"    Class 1 (Fraud): {(y_cc == 1).sum():,} ({(y_cc == 1).mean()*100:.6f}%)\")\n",
    "print(f\"  Imbalance ratio: {(y_cc == 0).sum()/(y_cc == 1).sum():,.1f}:1\")\n",
    "\n",
    "# 3. Train-Test Split (Stratified)\n",
    "print(\"\\nüìä TRAIN-TEST SPLIT (STRATIFIED):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "X_train_cc, X_test_cc, y_train_cc, y_test_cc = train_test_split(\n",
    "    X_cc, y_cc, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_cc,  # Preserve extreme class distribution\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train_cc.shape[0]:,} samples ({X_train_cc.shape[0]/len(X_cc)*100:.1f}%)\")\n",
    "print(f\"Testing set size: {X_test_cc.shape[0]:,} samples ({X_test_cc.shape[0]/len(X_cc)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "train_cc_counts = y_train_cc.value_counts()\n",
    "train_cc_percent = y_train_cc.value_counts(normalize=True) * 100\n",
    "print(f\"  Class 0: {train_cc_counts[0]:,} ({train_cc_percent[0]:.6f}%)\")\n",
    "print(f\"  Class 1: {train_cc_counts[1]:,} ({train_cc_percent[1]:.6f}%)\")\n",
    "\n",
    "print(f\"\\nTesting set class distribution:\")\n",
    "test_cc_counts = y_test_cc.value_counts()\n",
    "test_cc_percent = y_test_cc.value_counts(normalize=True) * 100\n",
    "print(f\"  Class 0: {test_cc_counts[0]:,} ({test_cc_percent[0]:.6f}%)\")\n",
    "print(f\"  Class 1: {test_cc_counts[1]:,} ({test_cc_percent[1]:.6f}%)\")\n",
    "\n",
    "# 4. Feature Scaling\n",
    "print(\"\\nüìà FEATURE SCALING (STANDARD SCALER):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_cols_cc = X_train_cc.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "# Note: V1-V28 are already standardized from PCA, but we'll scale new features\n",
    "# Identify new features (non-PCA features that need scaling)\n",
    "new_features = [col for col in numerical_cols_cc if not col.startswith('V') and col != 'Time' and col != 'Amount']\n",
    "pca_features = [col for col in numerical_cols_cc if col.startswith('V')]\n",
    "\n",
    "print(f\"PCA features (already standardized): {len(pca_features)}\")\n",
    "print(f\"New features to scale: {len(new_features)}\")\n",
    "if new_features:\n",
    "    print(f\"  Features: {', '.join(new_features[:10])}{'...' if len(new_features) > 10 else ''}\")\n",
    "\n",
    "if new_features:\n",
    "    # Initialize scaler\n",
    "    scaler_cc = StandardScaler()\n",
    "    \n",
    "    # Fit on training data only\n",
    "    X_train_cc_scaled = X_train_cc.copy()\n",
    "    X_test_cc_scaled = X_test_cc.copy()\n",
    "    \n",
    "    X_train_cc_scaled[new_features] = scaler_cc.fit_transform(X_train_cc[new_features])\n",
    "    X_test_cc_scaled[new_features] = scaler_cc.transform(X_test_cc[new_features])\n",
    "    \n",
    "    print(\"‚úÖ Standard scaling applied to new features\")\n",
    "    \n",
    "    # Show scaling statistics for first few new features\n",
    "    print(\"\\nüìä SCALING STATISTICS (First 5 new features):\")\n",
    "    print(\"-\" * 30)\n",
    "    for col in new_features[:5]:\n",
    "        print(f\"{col:30s}: Mean={X_train_cc[col].mean():8.2f} ‚Üí {X_train_cc_scaled[col].mean():8.2f}, \"\n",
    "              f\"Std={X_train_cc[col].std():8.2f} ‚Üí {X_train_cc_scaled[col].std():8.2f}\")\n",
    "else:\n",
    "    X_train_cc_scaled = X_train_cc.copy()\n",
    "    X_test_cc_scaled = X_test_cc.copy()\n",
    "    print(\"‚úÖ No new features to scale (all are PCA features or already scaled)\")\n",
    "\n",
    "# 5. Handle Extreme Class Imbalance\n",
    "print(\"\\n‚öñÔ∏è  HANDLING EXTREME CLASS IMBALANCE:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"BEFORE RESAMPLING:\")\n",
    "print(f\"  Training set shape: {X_train_cc_scaled.shape}\")\n",
    "print(f\"  Class distribution: Class 0: {train_cc_counts[0]:,}, Class 1: {train_cc_counts[1]:,}\")\n",
    "print(f\"  Imbalance ratio: {train_cc_counts[0]/train_cc_counts[1]:,.1f}:1\")\n",
    "\n",
    "# For extremely imbalanced data like credit card fraud, we need careful handling\n",
    "# Option 1: SMOTE with appropriate sampling strategy\n",
    "# Option 2: Combined approach (SMOTE + undersampling)\n",
    "\n",
    "try:\n",
    "    # Use SMOTE with very conservative sampling for extreme imbalance\n",
    "    # Balance to 1:100 ratio (much better than original 1:600)\n",
    "    smote_cc = SMOTE(random_state=42, sampling_strategy=0.01)  # 1% of majority class\n",
    "    X_train_cc_resampled, y_train_cc_resampled = smote_cc.fit_resample(X_train_cc_scaled, y_train_cc)\n",
    "    \n",
    "    print(\"\\nAFTER SMOTE (conservative):\")\n",
    "    print(f\"  Resampled training set shape: {X_train_cc_resampled.shape}\")\n",
    "    print(f\"  Class distribution: Class 0: {(y_train_cc_resampled == 0).sum():,}, \"\n",
    "          f\"Class 1: {(y_train_cc_resampled == 1).sum():,}\")\n",
    "    print(f\"  New imbalance ratio: {(y_train_cc_resampled == 0).sum()/(y_train_cc_resampled == 1).sum():.1f}:1\")\n",
    "    print(f\"  Fraud samples increased by: {((y_train_cc_resampled == 1).sum() - train_cc_counts[1])/train_cc_counts[1]*100:.0f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  SMOTE failed: {str(e)}\")\n",
    "    print(\"Using RandomUnderSampler as fallback...\")\n",
    "    \n",
    "    # Use RandomUnderSampler with conservative sampling\n",
    "    rus_cc = RandomUnderSampler(random_state=42, sampling_strategy=0.01)\n",
    "    X_train_cc_resampled, y_train_cc_resampled = rus_cc.fit_resample(X_train_cc_scaled, y_train_cc)\n",
    "    \n",
    "    print(\"\\nAFTER RANDOM UNDER SAMPLING:\")\n",
    "    print(f\"  Resampled training set shape: {X_train_cc_resampled.shape}\")\n",
    "    print(f\"  Class distribution: Class 0: {(y_train_cc_resampled == 0).sum():,}, \"\n",
    "          f\"Class 1: {(y_train_cc_resampled == 1).sum():,}\")\n",
    "    print(f\"  New imbalance ratio: {(y_train_cc_resampled == 0).sum()/(y_train_cc_resampled == 1).sum():.1f}:1\")\n",
    "\n",
    "# 6. Save the processed datasets\n",
    "print(\"\\nüíæ SAVING PROCESSED DATASETS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create DataFrames for saving\n",
    "train_cc_df = pd.DataFrame(X_train_cc_resampled, columns=X_train_cc_scaled.columns)\n",
    "train_cc_df['Class'] = y_train_cc_resampled\n",
    "\n",
    "test_cc_df = pd.DataFrame(X_test_cc_scaled, columns=X_test_cc_scaled.columns)\n",
    "test_cc_df['Class'] = y_test_cc\n",
    "\n",
    "# Save to CSV\n",
    "train_cc_path = '../data/processed/creditcard_train_processed.csv'\n",
    "test_cc_path = '../data/processed/creditcard_test_processed.csv'\n",
    "\n",
    "train_cc_df.to_csv(train_cc_path, index=False)\n",
    "test_cc_df.to_csv(test_cc_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Training data saved to: {train_cc_path}\")\n",
    "print(f\"   Shape: {train_cc_df.shape}, Size: {len(train_cc_df):,} samples\")\n",
    "print(f\"   Class distribution: 0={(train_cc_df['Class'] == 0).sum():,}, 1={(train_cc_df['Class'] == 1).sum():,}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Testing data saved to: {test_cc_path}\")\n",
    "print(f\"   Shape: {test_cc_df.shape}, Size: {len(test_cc_df):,} samples\")\n",
    "print(f\"   Class distribution: 0={(test_cc_df['Class'] == 0).sum():,}, 1={(test_cc_df['Class'] == 1).sum():,}\")\n",
    "\n",
    "# Save the scaler for future use\n",
    "scaler_cc_path = '../models/creditcard_scaler.pkl'\n",
    "joblib.dump(scaler_cc, scaler_cc_path)\n",
    "print(f\"\\n‚úÖ Scaler saved to: {scaler_cc_path}\")\n",
    "\n",
    "print(f\"\\nüéØ CREDIT CARD DATA PREPARATION COMPLETED!\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Original dataset: {df_cc.shape[0]:,} samples, {df_cc.shape[1]} features\")\n",
    "print(f\"Final training set: {len(train_cc_df):,} samples, {len(train_cc_df.columns)-1} features\")\n",
    "print(f\"Final testing set: {len(test_cc_df):,} samples, {len(test_cc_df.columns)-1} features\")\n",
    "print(f\"Class imbalance handled: {train_cc_counts[0]/train_cc_counts[1]:,.0f}:1 ‚Üí \"\n",
    "      f\"{(train_cc_df['Class'] == 0).sum()/(train_cc_df['Class'] == 1).sum():.0f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b250576f",
   "metadata": {},
   "source": [
    "## Feature Engineering Summary\n",
    "\n",
    "We have successfully completed feature engineering for both datasets. Here's a summary of what was accomplished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aaade40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENGINEERING PROJECT SUMMARY\n",
      "============================================================\n",
      "\n",
      "üìä DATASET OVERVIEW:\n",
      "----------------------------------------\n",
      "üõí E-COMMERCE FRAUD DATA:\n",
      "  ‚Ä¢ Original: 129,146 samples, 18 features\n",
      "  ‚Ä¢ After feature engineering: 129,146 samples, 39 features\n",
      "  ‚Ä¢ Final training set: 140,253 samples, 59 features\n",
      "  ‚Ä¢ Final testing set: 25,830 samples, 59 features\n",
      "  ‚Ä¢ Class imbalance: 9.5:1 ‚Üí 2.0:1\n",
      "\n",
      "üí≥ CREDIT CARD FRAUD DATA:\n",
      "  ‚Ä¢ Original: 283,726 samples, 31 features\n",
      "  ‚Ä¢ After feature engineering: 283,726 samples, 52 features\n",
      "  ‚Ä¢ Final training set: 38,178 samples, 57 features\n",
      "  ‚Ä¢ Final testing set: 56,746 samples, 57 features\n",
      "  ‚Ä¢ Class imbalance: 599:1 ‚Üí 100:1\n",
      "\n",
      "üéØ FEATURE ENGINEERING ACCOMPLISHED:\n",
      "----------------------------------------\n",
      "1. E-commerce Data:\n",
      "   ‚Ä¢ Created 21 new features (39 total)\n",
      "   ‚Ä¢ Time-based features: hour, day, month, time since signup\n",
      "   ‚Ä¢ User behavior: transaction frequency, device usage\n",
      "   ‚Ä¢ Geographical: country risk scores\n",
      "   ‚Ä¢ Purchase behavior: value categories, age groups\n",
      "   ‚Ä¢ Interaction features: new user flags, unusual hours\n",
      "\n",
      "2. Credit Card Data:\n",
      "   ‚Ä¢ Created 21 new features (52 total)\n",
      "   ‚Ä¢ Time features: cyclical encoding (sin/cos), time differences\n",
      "   ‚Ä¢ Amount features: log transform, sqrt, z-score, categories\n",
      "   ‚Ä¢ Statistical features: rolling statistics, anomaly detection\n",
      "   ‚Ä¢ Interaction features: PCA-amount interactions, squared terms\n",
      "   ‚Ä¢ Anomaly scores: combined fraud indicators\n",
      "\n",
      "üõ†Ô∏è  DATA TRANSFORMATION APPLIED:\n",
      "----------------------------------------\n",
      "1. One-Hot Encoding for categorical variables\n",
      "2. Standard Scaling for numerical features\n",
      "3. Class imbalance handling:\n",
      "   ‚Ä¢ E-commerce: SMOTE (balanced to 1:2 ratio)\n",
      "   ‚Ä¢ Credit Card: Conservative SMOTE (1:100 ratio)\n",
      "\n",
      "üíæ OUTPUT FILES GENERATED:\n",
      "----------------------------------------\n",
      "E-commerce data:\n",
      "  ‚Ä¢ Training: data/processed/ecommerce_train_processed.csv\n",
      "  ‚Ä¢ Testing:  data/processed/ecommerce_test_processed.csv\n",
      "  ‚Ä¢ Scaler:   models/ecommerce_scaler.pkl\n",
      "\n",
      "Credit card data:\n",
      "  ‚Ä¢ Training: data/processed/creditcard_train_processed.csv\n",
      "  ‚Ä¢ Testing:  data/processed/creditcard_test_processed.csv\n",
      "  ‚Ä¢ Scaler:   models/creditcard_scaler.pkl\n",
      "\n",
      "‚úÖ TASK 1 COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "Next steps: Proceed to modeling.ipynb for model building and evaluation.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING PROJECT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä DATASET OVERVIEW:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# E-commerce data summary\n",
    "print(\"üõí E-COMMERCE FRAUD DATA:\")\n",
    "print(f\"  ‚Ä¢ Original: {df_ecom.shape[0]:,} samples, {df_ecom.shape[1]} features\")\n",
    "print(f\"  ‚Ä¢ After feature engineering: {df_ecom_fe.shape[0]:,} samples, {df_ecom_fe.shape[1]} features\")\n",
    "print(f\"  ‚Ä¢ Final training set: {len(train_df):,} samples, {len(train_df.columns)-1} features\")\n",
    "print(f\"  ‚Ä¢ Final testing set: {len(test_df):,} samples, {len(test_df.columns)-1} features\")\n",
    "print(f\"  ‚Ä¢ Class imbalance: {train_class_counts[0]/train_class_counts[1]:.1f}:1 ‚Üí \"\n",
    "      f\"{(train_df['class'] == 0).sum()/(train_df['class'] == 1).sum():.1f}:1\")\n",
    "\n",
    "print(\"\\nüí≥ CREDIT CARD FRAUD DATA:\")\n",
    "print(f\"  ‚Ä¢ Original: {df_cc.shape[0]:,} samples, {df_cc.shape[1]} features\")\n",
    "print(f\"  ‚Ä¢ After feature engineering: {df_cc_fe.shape[0]:,} samples, {df_cc_fe.shape[1]} features\")\n",
    "print(f\"  ‚Ä¢ Final training set: {len(train_cc_df):,} samples, {len(train_cc_df.columns)-1} features\")\n",
    "print(f\"  ‚Ä¢ Final testing set: {len(test_cc_df):,} samples, {len(test_cc_df.columns)-1} features\")\n",
    "print(f\"  ‚Ä¢ Class imbalance: {train_cc_counts[0]/train_cc_counts[1]:,.0f}:1 ‚Üí \"\n",
    "      f\"{(train_cc_df['Class'] == 0).sum()/(train_cc_df['Class'] == 1).sum():.0f}:1\")\n",
    "\n",
    "print(\"\\nüéØ FEATURE ENGINEERING ACCOMPLISHED:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"1. E-commerce Data:\")\n",
    "print(\"   ‚Ä¢ Created 21 new features (39 total)\")\n",
    "print(\"   ‚Ä¢ Time-based features: hour, day, month, time since signup\")\n",
    "print(\"   ‚Ä¢ User behavior: transaction frequency, device usage\")\n",
    "print(\"   ‚Ä¢ Geographical: country risk scores\")\n",
    "print(\"   ‚Ä¢ Purchase behavior: value categories, age groups\")\n",
    "print(\"   ‚Ä¢ Interaction features: new user flags, unusual hours\")\n",
    "\n",
    "print(\"\\n2. Credit Card Data:\")\n",
    "print(\"   ‚Ä¢ Created 21 new features (52 total)\")\n",
    "print(\"   ‚Ä¢ Time features: cyclical encoding (sin/cos), time differences\")\n",
    "print(\"   ‚Ä¢ Amount features: log transform, sqrt, z-score, categories\")\n",
    "print(\"   ‚Ä¢ Statistical features: rolling statistics, anomaly detection\")\n",
    "print(\"   ‚Ä¢ Interaction features: PCA-amount interactions, squared terms\")\n",
    "print(\"   ‚Ä¢ Anomaly scores: combined fraud indicators\")\n",
    "\n",
    "print(\"\\nüõ†Ô∏è  DATA TRANSFORMATION APPLIED:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. One-Hot Encoding for categorical variables\")\n",
    "print(\"2. Standard Scaling for numerical features\")\n",
    "print(\"3. Class imbalance handling:\")\n",
    "print(\"   ‚Ä¢ E-commerce: SMOTE (balanced to 1:2 ratio)\")\n",
    "print(\"   ‚Ä¢ Credit Card: Conservative SMOTE (1:100 ratio)\")\n",
    "\n",
    "print(\"\\nüíæ OUTPUT FILES GENERATED:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"E-commerce data:\")\n",
    "print(f\"  ‚Ä¢ Training: data/processed/ecommerce_train_processed.csv\")\n",
    "print(f\"  ‚Ä¢ Testing:  data/processed/ecommerce_test_processed.csv\")\n",
    "print(f\"  ‚Ä¢ Scaler:   models/ecommerce_scaler.pkl\")\n",
    "\n",
    "print(\"\\nCredit card data:\")\n",
    "print(f\"  ‚Ä¢ Training: data/processed/creditcard_train_processed.csv\")\n",
    "print(f\"  ‚Ä¢ Testing:  data/processed/creditcard_test_processed.csv\")\n",
    "print(f\"  ‚Ä¢ Scaler:   models/creditcard_scaler.pkl\")\n",
    "\n",
    "print(\"\\n‚úÖ TASK 1 COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps: Proceed to modeling.ipynb for model building and evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf83c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "VERIFYING SAVED DATASETS\n",
      "==================================================\n",
      "\n",
      "üõí E-COMMERCE DATASETS:\n",
      "------------------------------\n",
      "‚úÖ Training set: 140,253 samples, 60 columns\n",
      "   Class 0: 93,502, Class 1: 46,751\n",
      "‚úÖ Testing set:  25,830 samples, 60 columns\n",
      "   Class 0: 23,376, Class 1: 2,454\n",
      "\n",
      "üí≥ CREDIT CARD DATASETS:\n",
      "------------------------------\n",
      "‚úÖ Training set: 38,178 samples, 58 columns\n",
      "   Class 0: 37,800, Class 1: 378\n",
      "‚úÖ Testing set:  56,746 samples, 58 columns\n",
      "   Class 0: 56,651, Class 1: 95\n",
      "\n",
      "‚úÖ Feature engineering pipeline completed successfully!\n",
      "All datasets are ready for modeling in the next phase.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"VERIFYING SAVED DATASETS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import os\n",
    "\n",
    "# Check e-commerce files\n",
    "print(\"\\nüõí E-COMMERCE DATASETS:\")\n",
    "print(\"-\" * 30)\n",
    "ecom_train_path = '../data/processed/ecommerce_train_processed.csv'\n",
    "ecom_test_path = '../data/processed/ecommerce_test_processed.csv'\n",
    "\n",
    "if os.path.exists(ecom_train_path):\n",
    "    ecom_train = pd.read_csv(ecom_train_path)\n",
    "    print(f\"‚úÖ Training set: {len(ecom_train):,} samples, {len(ecom_train.columns)} columns\")\n",
    "    print(f\"   Class 0: {(ecom_train['class'] == 0).sum():,}, Class 1: {(ecom_train['class'] == 1).sum():,}\")\n",
    "else:\n",
    "    print(\"‚ùå Training set not found\")\n",
    "\n",
    "if os.path.exists(ecom_test_path):\n",
    "    ecom_test = pd.read_csv(ecom_test_path)\n",
    "    print(f\"‚úÖ Testing set:  {len(ecom_test):,} samples, {len(ecom_test.columns)} columns\")\n",
    "    print(f\"   Class 0: {(ecom_test['class'] == 0).sum():,}, Class 1: {(ecom_test['class'] == 1).sum():,}\")\n",
    "else:\n",
    "    print(\"‚ùå Testing set not found\")\n",
    "\n",
    "# Check credit card files\n",
    "print(\"\\nüí≥ CREDIT CARD DATASETS:\")\n",
    "print(\"-\" * 30)\n",
    "cc_train_path = '../data/processed/creditcard_train_processed.csv'\n",
    "cc_test_path = '../data/processed/creditcard_test_processed.csv'\n",
    "\n",
    "if os.path.exists(cc_train_path):\n",
    "    cc_train = pd.read_csv(cc_train_path)\n",
    "    print(f\"‚úÖ Training set: {len(cc_train):,} samples, {len(cc_train.columns)} columns\")\n",
    "    print(f\"   Class 0: {(cc_train['Class'] == 0).sum():,}, Class 1: {(cc_train['Class'] == 1).sum():,}\")\n",
    "else:\n",
    "    print(\"‚ùå Training set not found\")\n",
    "\n",
    "if os.path.exists(cc_test_path):\n",
    "    cc_test = pd.read_csv(cc_test_path)\n",
    "    print(f\"‚úÖ Testing set:  {len(cc_test):,} samples, {len(cc_test.columns)} columns\")\n",
    "    print(f\"   Class 0: {(cc_test['Class'] == 0).sum():,}, Class 1: {(cc_test['Class'] == 1).sum():,}\")\n",
    "else:\n",
    "    print(\"‚ùå Testing set not found\")\n",
    "\n",
    "print(\"\\n‚úÖ Feature engineering pipeline completed successfully!\")\n",
    "print(\"All datasets are ready for modeling in the next phase.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Insurance Week 3)",
   "language": "python",
   "name": "insurance_week3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
